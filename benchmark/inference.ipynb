{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec488b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import tensorrt as trt\n",
    "import cv2 as cv\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from boxmot.trackers import ByteTrack, BoostTrack, BotSort, StrongSort, OcSort, DeepOcSort, HybridSort\n",
    "import time\n",
    "import logging\n",
    "import gc\n",
    "import json\n",
    "import pandas as pd \n",
    "import pathlib\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df9eefe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "MODEL_PATH = os.path.join(Path.cwd().parent, 'models', 'yolov7-tiny.engine')\n",
    "REID_PATH = os.path.join(Path.cwd().parent, 'models', 'osnet_x0_25_msmt17.pt')\n",
    "\n",
    "if not os.path.exists(MODEL_PATH):\n",
    "    raise FileNotFoundError(f\"Model file not found at {MODEL_PATH}\")\n",
    "\n",
    "if not os.path.exists(REID_PATH):\n",
    "    raise FileNotFoundError(f\"ReID model file not found at {REID_PATH}\")\n",
    "\n",
    "FRAMES_DIR = r'../video/frames'\n",
    "NUM_ROUNDS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17f0184d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_logger(name: str, level=logging.INFO):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    if not logger.handlers:\n",
    "        handler = logging.StreamHandler()\n",
    "        formatter = logging.Formatter(\"%(asctime)s | %(levelname)s | %(name)s | %(message)s\")\n",
    "        handler.setFormatter(formatter)\n",
    "        logger.addHandler(handler)\n",
    "    logger.propagate = False\n",
    "    return logger\n",
    "\n",
    "def cleanup():\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.ipc_collect()\n",
    "    gc.collect()\n",
    "\n",
    "logger = get_logger(\"Benchmark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66d143d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_NAME = \"images\"\n",
    "TRT_LOGGER = trt.Logger(trt.Logger.INFO)\n",
    "trt.init_libnvinfer_plugins(TRT_LOGGER, \"\")\n",
    "\n",
    "class TRTModel:\n",
    "    def __init__(self, engine_path: str, input_shape: tuple, device: torch.device):\n",
    "        self.input_shape = input_shape\n",
    "        self.device = torch.device(\"cuda:0\") if device.type == \"cuda\" else device\n",
    "        \n",
    "        torch.cuda.set_device(self.device)\n",
    "        self.trt_stream = torch.cuda.Stream(device=self.device)\n",
    "        \n",
    "        # Load engine\n",
    "        with open(engine_path, \"rb\") as f:\n",
    "            self.engine = trt.Runtime(TRT_LOGGER).deserialize_cuda_engine(f.read())\n",
    "        if self.engine is None:\n",
    "            raise RuntimeError(\"Failed to deserialize TensorRT engine\")\n",
    "        \n",
    "        self.context = self.engine.create_execution_context()\n",
    "        \n",
    "        # Setup input tensor\n",
    "        self.input_tensor = torch.empty(input_shape, device=self.device, dtype=torch.float32)\n",
    "        self.context.set_input_shape(INPUT_NAME, input_shape)\n",
    "        self.context.set_tensor_address(INPUT_NAME, int(self.input_tensor.data_ptr()))\n",
    "        \n",
    "        # Setup output tensors\n",
    "        self.outputs = {}\n",
    "        for i in range(self.engine.num_io_tensors):\n",
    "            name = self.engine.get_tensor_name(i)\n",
    "            if self.engine.get_tensor_mode(name) == trt.TensorIOMode.OUTPUT:\n",
    "                shape = tuple(self.context.get_tensor_shape(name))\n",
    "                dtype = trt.nptype(self.engine.get_tensor_dtype(name))\n",
    "                tensor = torch.empty(shape, device=self.device, \n",
    "                                   dtype=torch.from_numpy(np.empty((), dtype=dtype)).dtype)\n",
    "                self.outputs[name] = tensor\n",
    "                self.context.set_tensor_address(name, int(tensor.data_ptr()))\n",
    "        \n",
    "        self.warmup()\n",
    "\n",
    "    def warmup(self, iters: int = 30):\n",
    "        times = []\n",
    "        for _ in range(iters):\n",
    "            self.input_tensor.normal_()\n",
    "            starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "            starter.record(stream=self.trt_stream)\n",
    "            with torch.cuda.stream(self.trt_stream):\n",
    "                self.context.execute_async_v3(self.trt_stream.cuda_stream)\n",
    "            ender.record(stream=self.trt_stream)\n",
    "            self.trt_stream.synchronize()\n",
    "            times.append(starter.elapsed_time(ender) / 1000.0)\n",
    "        logger.info(f\"Warmup: {sum(times)/len(times):.4f}s avg inference time\")\n",
    "\n",
    "    def infer(self, inp: torch.Tensor) -> tuple:\n",
    "        assert inp.device == self.device and inp.shape == self.input_shape\n",
    "        \n",
    "        starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "        with torch.cuda.stream(self.trt_stream):\n",
    "            self.input_tensor.copy_(inp, non_blocking=True)\n",
    "            starter.record(stream=self.trt_stream)\n",
    "            self.context.execute_async_v3(self.trt_stream.cuda_stream)\n",
    "            ender.record(stream=self.trt_stream)\n",
    "        \n",
    "        self.trt_stream.synchronize()\n",
    "        return starter.elapsed_time(ender) / 1000.0, self.outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27a3a325",
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox(im, new_shape=(640, 640), color=(114, 114, 114)):\n",
    "    shape = im.shape[:2]\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    \n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = (new_shape[1] - new_unpad[0]) / 2, (new_shape[0] - new_unpad[1]) / 2\n",
    "    \n",
    "    if shape[::-1] != new_unpad:\n",
    "        im = cv.resize(im, new_unpad, interpolation=cv.INTER_LINEAR)\n",
    "    \n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv.copyMakeBorder(im, top, bottom, left, right, cv.BORDER_CONSTANT, value=color)\n",
    "    return im, r, (dw, dh)\n",
    "\n",
    "def scale_coords_back(boxes, ratio, dwdh):\n",
    "    \"\"\"\n",
    "    Scale bounding boxes from 640x640 letterboxed space back to original frame space\n",
    "    \n",
    "    Args:\n",
    "        boxes: numpy array of shape (N, 4) with [x1, y1, x2, y2] in 640x640 space\n",
    "        ratio: scaling ratio from letterbox\n",
    "        dwdh: (dw, dh) padding offsets from letterbox\n",
    "    \n",
    "    Returns:\n",
    "        scaled boxes in original frame coordinates\n",
    "    \"\"\"\n",
    "    dw, dh = dwdh\n",
    "    \n",
    "    # Remove padding\n",
    "    boxes = boxes.copy()\n",
    "    boxes[:, [0, 2]] -= dw  # x coords\n",
    "    boxes[:, [1, 3]] -= dh  # y coords\n",
    "    \n",
    "    # Scale by ratio\n",
    "    boxes /= ratio\n",
    "    \n",
    "    return boxes\n",
    "\n",
    "class VideoFrameDataset(Dataset):\n",
    "    def __init__(self, frames_dir, skip=0):\n",
    "        \"\"\"\n",
    "        Dataset for pre-extracted frames\n",
    "        Args:\n",
    "            frames_dir: Path to directory containing frame images\n",
    "            skip: Skip every nth frame (0 = no skip)\n",
    "        \"\"\"\n",
    "        self.frames_paths = []\n",
    "        self.skip = skip\n",
    "        \n",
    "        logger.info(f\"Loading frames from: {frames_dir}\")\n",
    "        \n",
    "        # Get all image files (sorted by name)\n",
    "        valid_extensions = ('.jpg', '.jpeg', '.png', '.bmp')\n",
    "        all_files = sorted([\n",
    "            os.path.join(frames_dir, f) \n",
    "            for f in os.listdir(frames_dir) \n",
    "            if f.lower().endswith(valid_extensions)\n",
    "        ])\n",
    "        \n",
    "        # Apply skip filter\n",
    "        for i, path in enumerate(all_files):\n",
    "            if skip != 0 and i % skip == 0:\n",
    "                continue\n",
    "            self.frames_paths.append(path)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.frames_paths)} frame paths\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.frames_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Read frame from disk\n",
    "        img_bgr = cv.imread(self.frames_paths[idx])\n",
    "        if img_bgr is None:\n",
    "            raise RuntimeError(f\"Failed to read frame: {self.frames_paths[idx]}\")\n",
    "        \n",
    "        img_rgb = cv.cvtColor(img_bgr, cv.COLOR_BGR2RGB)\n",
    "        img_lb, ratio, dwdh = letterbox(img_rgb, new_shape=(640, 640))\n",
    "        img_lb_bgr = cv.cvtColor(img_lb, cv.COLOR_RGB2BGR)\n",
    "        \n",
    "        img_chw = img_lb.transpose(2, 0, 1)\n",
    "        img_chw = np.ascontiguousarray(img_chw, dtype=np.float32)\n",
    "        tensor = torch.from_numpy(img_chw) / 255.0\n",
    "        \n",
    "        # Return original image to keep coordinates consistent with boxes scaled back\n",
    "        return img_bgr, tensor, ratio, dwdh, self.frames_paths[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    imgs, tensors, ratios, dwdhs, frame_paths = zip(*batch)\n",
    "    tensors = torch.stack(tensors, dim=0)\n",
    "    return imgs, tensors, ratios, dwdhs, frame_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a8e3dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trackers_config = {\n",
    "    'OcSort': {\n",
    "        'class': OcSort,\n",
    "        'params': {\n",
    "            'reid_weights': Path('osnet_x0_25_msmt17.pt'),\n",
    "            'device': device,\n",
    "            'half': False\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c8dc663",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-17 15:47:06,229 | INFO | Benchmark | Warmup: 0.0071s avg inference time\n",
      "2026-01-17 15:47:06,229 | INFO | Benchmark | Loading frames from: ../video/frames\n",
      "2026-01-17 15:47:06,236 | INFO | Benchmark | Loaded 1501 frame paths\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "model = TRTModel(\n",
    "    engine_path=MODEL_PATH,\n",
    "    input_shape=(1, 3, 640, 640),\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "# Load dataset from pre-extracted frames\n",
    "dataset = VideoFrameDataset(\n",
    "    frames_dir=FRAMES_DIR,\n",
    "    skip=0,  # adjust if you want to skip frames\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fc7c2a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "NOTEBOOK_DIR = Path.cwd()\n",
    "DATA_DIR = NOTEBOOK_DIR / \"data\"\n",
    "PREDICTED_DIR = DATA_DIR / \"predicted\"\n",
    "CONFIG_DIR = DATA_DIR / \"config\"\n",
    "PREDICTED_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CONFIG_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save results directory with timestamp\n",
    "directory_time = input(\"Enter a name for the results directory (or press Enter to use timestamp): \")\n",
    "if directory_time.strip() == \"\":\n",
    "    RUNNING_TIME = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "else:\n",
    "    RUNNING_TIME = directory_time.strip()\n",
    "RUN_RESULTS_DIR = PREDICTED_DIR / RUNNING_TIME\n",
    "RUN_RESULTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "RUN_CONFIG_DIR = CONFIG_DIR / RUNNING_TIME\n",
    "RUN_CONFIG_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "212b4840",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-17 15:47:12,318 | INFO | Benchmark | \n",
      "============================================================\n",
      "2026-01-17 15:47:12,318 | INFO | Benchmark | OcSort | Attempt 1 of 1\n",
      "2026-01-17 15:47:12,320 | INFO | Benchmark | ============================================================\n",
      "OcSort (run 1):   0%|          | 4/1501 [00:00<01:47, 13.93it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "only 0-dimensional arrays can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 47\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# Tracking\u001b[39;00m\n\u001b[32m     46\u001b[39m start = time.perf_counter()\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m results = \u001b[43mtracker\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m tracking_time = time.perf_counter() - start\n\u001b[32m     49\u001b[39m tracking_times.append(tracking_time)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Auto\\anaconda3\\envs\\traffic\\Lib\\site-packages\\boxmot\\trackers\\basetracker.py:194\u001b[39m, in \u001b[36mBaseTracker.setup_decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    191\u001b[39m     \u001b[38;5;28mself\u001b[39m._first_frame_processed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Call the original method with the unwrapped `dets`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Auto\\anaconda3\\envs\\traffic\\Lib\\site-packages\\boxmot\\trackers\\basetracker.py:211\u001b[39m, in \u001b[36mBaseTracker.per_class_decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, dets, img, embs)\u001b[39m\n\u001b[32m    207\u001b[39m     dets = np.empty((\u001b[32m0\u001b[39m, \u001b[32m6\u001b[39m))\n\u001b[32m    209\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.per_class:\n\u001b[32m    210\u001b[39m     \u001b[38;5;66;03m# Process all detections at once if per_class is False\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m211\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mupdate_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdets\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membs\u001b[49m\u001b[43m=\u001b[49m\u001b[43membs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    212\u001b[39m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[32m    213\u001b[39m \u001b[38;5;66;03m# Initialize an array to store the tracks for each class\u001b[39;00m\n\u001b[32m    214\u001b[39m per_class_tracks = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Auto\\anaconda3\\envs\\traffic\\Lib\\site-packages\\boxmot\\trackers\\ocsort\\ocsort.py:381\u001b[39m, in \u001b[36mOcSort.update\u001b[39m\u001b[34m(self, dets, img, embs)\u001b[39m\n\u001b[32m    379\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m iou_left[m[\u001b[32m0\u001b[39m], m[\u001b[32m1\u001b[39m]] < \u001b[38;5;28mself\u001b[39m.asso_threshold:\n\u001b[32m    380\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m381\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactive_tracks\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrk_ind\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdet_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdet_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdet_ind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m    383\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    384\u001b[39m to_remove_det_indices.append(det_ind)\n\u001b[32m    385\u001b[39m to_remove_trk_indices.append(trk_ind)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Auto\\anaconda3\\envs\\traffic\\Lib\\site-packages\\boxmot\\trackers\\ocsort\\ocsort.py:168\u001b[39m, in \u001b[36mKalmanBoxTracker.update\u001b[39m\u001b[34m(self, bbox, cls, det_ind)\u001b[39m\n\u001b[32m    166\u001b[39m     \u001b[38;5;28mself\u001b[39m.hits += \u001b[32m1\u001b[39m\n\u001b[32m    167\u001b[39m     \u001b[38;5;28mself\u001b[39m.hit_streak += \u001b[32m1\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m168\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxyxy2xysr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbbox\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    170\u001b[39m     \u001b[38;5;28mself\u001b[39m.kf.update(bbox)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Auto\\anaconda3\\envs\\traffic\\Lib\\site-packages\\boxmot\\motion\\kalman_filters\\aabb\\xysr_kf.py:253\u001b[39m, in \u001b[36mKalmanFilterXYSR.update\u001b[39m\u001b[34m(self, z, R, H)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.observed:\n\u001b[32m    250\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[33;03m    Get observation, use online smoothing to re-update parameters\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43munfreeze\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[38;5;28mself\u001b[39m.observed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m R \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Auto\\anaconda3\\envs\\traffic\\Lib\\site-packages\\boxmot\\motion\\kalman_filters\\aabb\\xysr_kf.py:203\u001b[39m, in \u001b[36mKalmanFilterXYSR.unfreeze\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m x, y = x1 + (i + \u001b[32m1\u001b[39m) * dx, y1 + (i + \u001b[32m1\u001b[39m) * dy\n\u001b[32m    202\u001b[39m w, h = w1 + (i + \u001b[32m1\u001b[39m) * dw, h1 + (i + \u001b[32m1\u001b[39m) * dh\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m s, r = w * h, w / \u001b[38;5;28mfloat\u001b[39m(h)\n\u001b[32m    204\u001b[39m new_box = np.array([x, y, s, r]).reshape((\u001b[32m4\u001b[39m, \u001b[32m1\u001b[39m))\n\u001b[32m    205\u001b[39m \u001b[38;5;28mself\u001b[39m.update(new_box)\n",
      "\u001b[31mTypeError\u001b[39m: only 0-dimensional arrays can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "all_results = []\n",
    "\n",
    "for tracker_name, config in trackers_config.items():\n",
    "    for round_idx in range(NUM_ROUNDS):\n",
    "        logger.info(f\"\\n{'='*60}\")\n",
    "        logger.info(f\"{tracker_name} | Attempt {round_idx + 1} of {NUM_ROUNDS}\")\n",
    "        logger.info(f\"{'='*60}\")\n",
    "\n",
    "        tracker = config[\"class\"](**config[\"params\"])\n",
    "        tracker.reset()\n",
    "\n",
    "        infer_times = []\n",
    "        tracking_times = []\n",
    "        tracking_rows = []\n",
    "\n",
    "        for frame_idx, (imgs, input_tensor, ratios, dwdhs, frame_paths) in enumerate(\n",
    "            tqdm(dataloader, desc=f\"{tracker_name} (run {round_idx + 1})\")\n",
    "        ):\n",
    "            input_tensor = input_tensor.to(device)\n",
    "\n",
    "            # Inference\n",
    "            infer_time, outputs = model.infer(input_tensor)\n",
    "            infer_times.append(infer_time)\n",
    "\n",
    "            # Parse detections\n",
    "            num = int(outputs[\"num_dets\"][0].item())\n",
    "            boxes = outputs[\"det_boxes\"][0][:num].cpu().numpy()\n",
    "            scores = outputs[\"det_scores\"][0][:num].cpu().numpy()\n",
    "            classes = outputs[\"det_classes\"][0][:num].cpu().numpy()\n",
    "\n",
    "            # Scale boxes back to original frame coordinates\n",
    "            boxes_original = scale_coords_back(boxes, ratios[0], dwdhs[0])\n",
    "\n",
    "            # Clamp boxes to original image bounds and filter invalid boxes\n",
    "            h, w = imgs[0].shape[:2]\n",
    "            boxes_original[:, [0, 2]] = np.clip(boxes_original[:, [0, 2]], 0, w - 1)\n",
    "            boxes_original[:, [1, 3]] = np.clip(boxes_original[:, [1, 3]], 0, h - 1)\n",
    "            valid = (boxes_original[:, 2] > boxes_original[:, 0] + 1) & (boxes_original[:, 3] > boxes_original[:, 1] + 1)\n",
    "            boxes_original = boxes_original[valid]\n",
    "            scores = scores[valid]\n",
    "            classes = classes[valid]\n",
    "\n",
    "            dets = np.concatenate([boxes_original, scores[:, None], classes[:, None]], axis=-1)\n",
    "\n",
    "            # Tracking\n",
    "            start = time.perf_counter()\n",
    "            results = tracker.update(dets, imgs[0])\n",
    "            tracking_time = time.perf_counter() - start\n",
    "            tracking_times.append(tracking_time)\n",
    "\n",
    "            id_map = {}\n",
    "            next_id = 1\n",
    "\n",
    "            # Store: frame, id, left, top, width, height, confidence\n",
    "            for track in results:\n",
    "                x1, y1, x2, y2, track_id = track[:5]\n",
    "                conf = track[5] if len(track) > 5 else 1.0\n",
    "\n",
    "                if track_id not in id_map:\n",
    "                    id_map[track_id] = next_id\n",
    "                    next_id += 1\n",
    "                mapped_id = id_map[track_id]\n",
    "\n",
    "                width = x2 - x1\n",
    "                height = y2 - y1\n",
    "                tracking_rows.append(\n",
    "                    {\n",
    "                        \"frame\": frame_idx+1,\n",
    "                        \"id\": int(mapped_id),\n",
    "                        \"bb_left\": float(x1),\n",
    "                        \"bb_top\": float(y1),\n",
    "                        \"bb_width\": float(width),\n",
    "                        \"bb_height\": float(height),\n",
    "                        \"conf\": float(conf),\n",
    "                        \"x\": int(-1),\n",
    "                        \"y\": int(-1),\n",
    "                        \"z\": int(-1),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Paths for this round (ensure directories exist)\n",
    "        csv_path = RUN_RESULTS_DIR / f\"{tracker_name}_round{round_idx + 1}.csv\"\n",
    "        json_path = RUN_CONFIG_DIR / f\"{tracker_name}_round{round_idx + 1}.json\"\n",
    "\n",
    "        # Save CSV\n",
    "        pd.DataFrame(tracking_rows).to_csv(csv_path, index=False)\n",
    "\n",
    "        # Save JSON summary\n",
    "        summary = {\n",
    "            \"attempt\": round_idx + 1,\n",
    "            \"algorithm\": tracker_name,\n",
    "            \"input_path\": FRAMES_DIR,\n",
    "            \"inference_time\": infer_times,\n",
    "            \"tracking_time\": tracking_times,\n",
    "            \"path_to_detections\": str(csv_path),\n",
    "        }\n",
    "        with open(json_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "\n",
    "        all_results.append(summary)\n",
    "        logger.info(\n",
    "            f\"{tracker_name} (run {round_idx + 1}) \"\n",
    "            f\"- frames: {len(infer_times)} \"\n",
    "            f\"- avg inf: {np.mean(infer_times):.4f}s \"\n",
    "            f\"- avg track: {np.mean(tracking_times):.4f}s \"\n",
    "            f\"- csv: {csv_path}\"\n",
    "        )\n",
    "\n",
    "cleanup()\n",
    "logger.info(\"\\nâœ“ All benchmarks complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdc4d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_textfile(df, file_name, directory=RUN_RESULTS_DIR):\n",
    "    try:\n",
    "        with open(os.path.join(directory, file_name), 'w') as f:\n",
    "            for row in df.iterrows():\n",
    "                values = row[1].tolist()\n",
    "                frame = int(values[0])\n",
    "                obj_id = int(values[1])\n",
    "                bb_left = float(values[2])\n",
    "                bb_top = float(values[3])\n",
    "                bb_width = float(values[4])\n",
    "                bb_height = float(values[5])\n",
    "                conf = int(values[6])\n",
    "                x = int(values[7])\n",
    "                y = int(values[8])\n",
    "                z = int(values[9]) \n",
    "                \n",
    "                line = f\"{frame},{obj_id},{bb_left},{bb_top},{bb_width},{bb_height},{conf},{x},{y},{z}\\n\"\n",
    "                f.write(line)\n",
    "\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f'Write error: {file_name} because {e}')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b499264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoostTrack_round1.csv already convert to txt format\n",
      "BoostTrack_round1.txt already convert to txt format\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir(RUN_RESULTS_DIR):\n",
    "    if file.split('.')[-1] == 'csv':\n",
    "        df = pd.read_csv(os.path.join(RUN_RESULTS_DIR, file))\n",
    "\n",
    "    if not os.path.exists(os.path.join(RUN_RESULTS_DIR, file.replace('.csv', '.txt'))):\n",
    "        write_textfile(df, file.replace('.csv', '.txt'), directory=RUN_RESULTS_DIR)\n",
    "    else:\n",
    "        print(f\"{file} already convert to txt format\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "traffic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
